{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alfredo is talking about dimensional reduction\n",
    "He says that PCA preserves structure while reducing data\n",
    "For instance you can map 3 dimensions into 2 dimensions\n",
    "This dimensional reduction has to be played with to see what it represents. \n",
    "If you keep the components high you may be able to get more and more dimensionality. \n",
    "\n",
    "$$x_{t+1} = A x_{t} $$\n",
    "\n",
    "Where alpha is a vector telling you how to move through the space, for instance height and width. \n",
    "\n",
    "An eigenvector of any matrix is the transformation where it will still go to the same direction. There is a set of vectors which after transform, remain in the same direction. In a dynamical system there are directions which are dominant, if you get the eigen vector decomposition of the original matrix then you will get the general trendiness of the model. \n",
    "\n",
    "When you do PCA, you imagine that the covariance matrix is giving you directions of where you would go. So regardless of whether you started with a very homgeneous circle. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

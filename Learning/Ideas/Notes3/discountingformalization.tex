\documentclass[11pt]{article}
%\documentclass[12pt]{article}
%\documentclass[12pt]{article}
%\documentclass[12pt,a4paper]{article}

\usepackage[percent]{overpic}
\usepackage{float}
\usepackage{pgfplots}
%\usepackage[cmbold]{mathtime}
%\usepackage{mt11p}
\usepackage{placeins}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{color}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{epsfig}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{rotating,tabularx}
%\usepackage[graphicx]{realboxes}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{epstopdf}
\usepackage{longtable}
\usepackage[pdftex]{hyperref}
%\usepackage{breakurl}
\usepackage{epigraph}
\usepackage{xspace}
\usepackage{amsfonts}
\usepackage{eurosym}
\usepackage{ulem}
\usepackage{footmisc}
\usepackage{comment}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{caption}
\usepackage{pdflscape}
\usepackage{array}
\usepackage[round]{natbib}
\usepackage{booktabs}
\usepackage{dcolumn}
\usepackage{mathrsfs}
%\usepackage[justification=centering]{caption}
%\captionsetup[table]{format=plain,labelformat=simple,labelsep=period,singlelinecheck=true}%

%\bibliographystyle{unsrtnat}
\bibliographystyle{aea}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
%\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
%\usepackage{tikz}
%\usetikzlibrary{snakes}
%\usetikzlibrary{patterns}

%\draftSpacing{1.5}

\usepackage{xcolor}
\hypersetup{
colorlinks,
linkcolor={blue!50!black},
citecolor={blue!50!black},
urlcolor={blue!50!black}}

%\renewcommand{\familydefault}{\sfdefault}
%\usepackage{helvet}
%\setlength{\parindent}{0.4cm}
%\setlength{\parindent}{2em}
%\setlength{\parskip}{1em}

%\normalem

%\doublespacing
\onehalfspacing
%\singlespacing
%\linespread{1.5}

\newtheorem{theorem}{Theorem}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{axiom}{Axiom}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Corr}{\mathrm{Corr}}
\newcommand{\Cov}{\mathrm{Cov}}

\newcolumntype{d}[1]{D{.}{.}{#1}} % "decimal" column type
\renewcommand{\ast}{{}^{\textstyle *}} % for raised "asterisks"

\newtheorem{hyp}{Hypothesis}
\newtheorem{subhyp}{Hypothesis}[hyp]
\renewcommand{\thesubhyp}{\thehyp\alph{subhyp}}

\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}

%\newcommand*{\qed}{\hfill\ensuremath{\blacksquare}}%

\newcolumntype{L}[1]{>{\raggedright\let\newline\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\arraybackslash\hspace{0pt}}m{#1}}

%\geometry{left=1.5in,right=1.5in,top=1.5in,bottom=1.5in}
\geometry{left=1in,right=1in,top=1in,bottom=1in}

\epstopdfsetup{outdir=./}

\newcommand{\elabel}[1]{\label{eq:#1}}
\newcommand{\eref}[1]{Eq.~(\ref{eq:#1})}
\newcommand{\ceref}[2]{(\ref{eq:#1}#2)}
\newcommand{\Eref}[1]{Equation~(\ref{eq:#1})}
\newcommand{\erefs}[2]{Eqs.~(\ref{eq:#1}--\ref{eq:#2})}

\newcommand{\Sref}[1]{Section~\ref{sec:#1}}
\newcommand{\sref}[1]{Sec.~\ref{sec:#1}}

\newcommand{\Pref}[1]{Proposition~\ref{prop:#1}}
\newcommand{\pref}[1]{Prop.~\ref{prop:#1}}
\newcommand{\preflong}[1]{proposition~\ref{prop:#1}}

\newcommand{\Aref}[1]{Axiom~\ref{ax:#1}}

\newcommand{\clabel}[1]{\label{coro:#1}}
\newcommand{\Cref}[1]{Corollary~\ref{coro:#1}}
\newcommand{\cref}[1]{Cor.~\ref{coro:#1}}
\newcommand{\creflong}[1]{corollary~\ref{coro:#1}}

\newcommand{\etal}{{\it et~al.}\xspace}
\newcommand{\ie}{{\it i.e.}\xspace}
\newcommand{\eg}{{\it e.g.}\xspace}
\newcommand{\etc}{{\it etc.}\xspace}
\newcommand{\cf}{{\it c.f.}\xspace}
\newcommand{\ave}[1]{\left\langle#1 \right\rangle}
\newcommand{\person}[1]{{\it \sc #1}}

\newcommand{\AAA}[1]{\red{{\it AA: #1 AA}}}
\newcommand{\YB}[1]{\blue{{\it YB: #1 YB}}}

\newcommand{\flabel}[1]{\label{fig:#1}}
\newcommand{\fref}[1]{Fig.~\ref{fig:#1}}
\newcommand{\Fref}[1]{Figure~\ref{fig:#1}}

\newcommand{\tlabel}[1]{\label{tab:#1}}
\newcommand{\tref}[1]{Tab.~\ref{tab:#1}}
\newcommand{\Tref}[1]{Table~\ref{tab:#1}}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}

\newcommand{\Dt}{\Delta t}
\newcommand{\Dx}{\Delta x}
\newcommand{\Epsilon}{\mathcal{E}}
\newcommand{\etau}{\tau^\text{eqm}}
\newcommand{\wtau}{\widetilde{\tau}}
\newcommand{\xN}{\ave{x}_N}
\newcommand{\Sdata}{S^{\text{data}}}
\newcommand{\Smodel}{S^{\text{model}}}

\newcommand{\del}{D}
\newcommand{\hor}{H}
\newcommand{\subhead}[1]{\mbox{}\newline\textbf{#1}\newline}

\setlength{\parindent}{0.0cm}
\setlength{\parskip}{0.5em}

\numberwithin{equation}{section}
\DeclareMathOperator\erf{erf}
%\let\endtitlepage\relax

\begin{document}

Suppose we have a list of objects. These objects can have various characteristics (color, size, shape, etc). Suppose that we come up with a category that fits into one of these characteristics, such as the category "red" which fits into the characteristic "color" or category "cube" which fits into the characteristic shape. We then go and test this category on the objects to see if the object fits into it. If the object fits into the category, we assign it that characteristic-category, "the color of this object is 'red'". Notice that so far, no prediction has been made, we have merely found ways to describe the objects. 

The empiricist will use these characteristics and categories. The empiricist will try to find the frequency of the categories by testing groups of objects. The statement she will make is of the form "30\% of objects have the color red". She can perhaps also make conditional statements "of the objects which are shaped 'cube', 40\% have the color 'red'". These conditional statements can also represent sampling styles, for instance, "If I select objects using a magnet, then 20\% will have the color 'red'". The empiricist can aim to have predictive power by making statements of the form "The objects sampled with magnets had a lower probability of having the color 'red', therefore we expect sampling using magnets to yield a lower proportion with the color 'red'." In essence, the role of the empiricist is finding correlations between categories.

The theorist could have more value-added. In this story, the theorist merely imagines pairs of characteristics-categories and sends them to the empiricist to see if they fit. However, the role of the theorist could be to rule out relationships between characteristics-categories. The utility of a theorist is maximized when he predicts a single value for each group of categories. If we imagine a vector of characteristics for each object [color, size, shape, etc]. A single valued predictive theory would allow us to take some categories [large, cube] and allow us to predict a single category, say, [red]. A theory does not have to be single valued but a single valued theory has the maximum amount of falsifiability. 

The axiomatic approach to decision theory does offer a sort of theory by linking behaviors to other behaviors. For instance we expect someone who meets the VNM axioms to exhibit some specific form of linear behavior. If they fail to exhibit this linear behavior we assume that one of the axioms must be violated, and test them individually. If they exhibit the linear behavior but violate an individual axiom we then try and replace that axiom with some other axiom that will give the same result but will not be violated. At the end of the process we should be able to group proportions of the populations into axiomatizations. If we found 3 axiomatizations which empirically explain the behavior of agents, the empiricist would simply say what proportion of individuals have correspond to what axiomatiation. 

If one axiomatization can explain numerous types of reactions then it usually has some kind of parameter that make the behavior emerge. For instance in Rhodes 2017, she has the 'decreasing impatience'. A question that remains is how one axiom can explain numerous types of behaviors? 

This way of doing theory is solid but essentially to test the axiomatizations there are certain data requirements. Suppose we know that three axioms imply a fourth one, [1,2,3] $\rightarrow [4]$. This means that we must have three data points for each prediction. 

Current theory sometimes does this, they say if you have property 1 and property 2, you should have some other property. But this does not explain WHY person A has property 1 and person B has property 2. 

So current paradigm does two things, it tries to explain variation of behavior within a single individuals actions. And it attempts to find frequencies and correlations between the sample. The purely empirical approach attempts to find 

The role of the theorist is then is to rule out some variations. If the theorist is predicting a single value for each state, like say, the categories [large, cube]-->[red], he is explicitely ruling out [large, cube]-->[blue]. 

This should fully explain the variability 

For the theorist to be useful, he must find a model which predicts the variability between and within objects. For instance  

around the whole setup. For instance, he could say that "because the color red has these characteristics, it is less attracted by magnetic forces than other colors". In other words, the theorist will try to explain variation \text{within} the sample of objects. 

All of this holds for decision theory. The traditional approach is to posit a few axioms, and then see what proportion of the sample violates each axiom, for example in the Von Neuman Morgensten axiomatization we usually find a higher frequency of agents violating the independence axiom. This whole approach does not explain \text{within} sample variation. Indeed it merely gives frequencies of the categories given. Indeed the innovation in the model presented here is that we can explain why an agent within category x will take action y. It is a casual approach to science. 

%%%%%%%%%%%%%%%%%%%%%%


Suppose we have two objects. We notice that either both objects are red or both objects are blue. This is not a theory but a description. What if we noticed that 50\% of objects are red and 50\% are blue. The is still not a theory but a description. A theory would be either to search for conditions under which the regularity of 50 \% will not occur OR to search for conditions that predict, even probabilistically, when each object will take on what state. 

We could try and see if this result we have found holds for other groups(science). If it does, and it applies to all groups, we have some reason of thinking we have found some kind of empirical regularity. However if it varies between groups, then the explanation must focus on new factors to explain the differences between groups. We have to richen the analysis. Maybe we can find a variable to explain the differences between groups and other factors for explaining the differences within groups. But an even better theory would be a factor that explains both differences between groups and differences within groups.  

If we have that the one on the left always turns blue, and the one on the right always turns left, this is again a description. Instead a theory would look at other characteristics that predict the same behavior. Perhaps the one on the left is larger, or perhaps it is more transparent, we can then attempt to generalize this to other domains. 

The issue with most of the literature is that it differentiates agents by their preferences. However this creates an empirical problem, mainly the problem of tautology. In other words, if two agents have a different measurement, it is generally said (tautologically) that they have different preferences. The problem is that this creates no measurement that could predict follow up behavior. If the behavior we are trying to explain is preferences over $(t,x)>(s,y)$, then we could posit a source for this preference. However since there is no source posited, there is simply no way to evaluate whether the system is useful. Science is about conditional prediction, if this, then that. However this basic framework offers no observable or measurable quantity. 

What is usually done is to define the set, and define what different concepts mean, such as monotonicity, and then ask if the concepts are falsified, for instance is the concept of monotonicity falsified or not. https://pubsonline.informs.org/doi/pdf/10.1287/mnsc.2017.3015
There is nothing particularly wrong with this but it does mean that there is an attempt are purely qualitative theories. However there is no attempt to link the falsification with a specific condition. The idea is to find a category where the variation in that category predicts which domain we are in. For instance suppose we have that agents display monotonicity with 50 but not with 40, should not the theory take this into account? It would perhaps be common sense to state that the accuracy of monotonicity goes up as a decreasing function of the agents wealth. 

The issue is that every agent is assumed to have his own theory. This is fundamentally unworkable, the analogy in physics is every particle having its own behavior. 

Having a theory for each individual might be interesting in it's own right but what is perhaps more interesting to economics is the unity among individuals. In other words, what measure can explain the differences in behavior \textit{between} individuals. 

There is some phenomenon we are interested in, and we are trying to find the conditions under which this phenomenon exists. 

Occams razor. Suppose we ask an individual if he prefers 100 now or 100 later. The least assumption hypothesis is that they discount. So we throw that away. Now the most falsifiable is that they discount in a straight line or that they discount by a constant fraction, both of these require just three point. This is also quickly falsified. 

Second fact: they discount more earlier than later
Third fact: They change their preferences depending on when asked

At this point it may be impossible to find a theory that explains individual behaviors. But it may be possible to find a theory between behaviors.  

Theory isn't about empirical regularities but about search for conditions under which some phenomenon will exist. 

In theory exponential discounting is more scientific than hyperbolic because it requires only two measurement points. However once this is falsified, we must move on to less measurable quantities such as hyperbolic discounting. Alternatively we can specify the conditions under which exponential discounting will occur more scientifically. In a manner of $if x then y$. Since we are conditioning the conditions under which it will occur.

This is also the issue with much of game theory, whenever there are multiple equilibria for a given parametrization, this precicts numerous outcomes. Now it may be that the distribution we would expect is the same which would not make it an issue, but if we can expect everyone to either do something or not do something then this is scientifically unworkable. 

T is a nondegenerate closed subinterval of $(0,\infty)$
X is a convex set of $\mathbb{R}^m$ containing $x=0$
$\succeq$ is a continous weak order. And $\preceq, \prec, \succ, \sim$ as usual. Preferences are determined by two pairs, we say that preferences are if the choice is $((t,x),(t',x')) \succeq ((t,x),(t'',x''))$. 
Monotonicity holds if $((t,x),(t',x')) \succeq ((t,x),(t',x''))$ implies $x'>x''$ $\forall t, t' \in T $
Impatience holds if for all $s<t$ 

\end{document}